exp000: full-precision, lr=1e-3, lrSchedule (cyclic)                   -->  6.40%

exp010: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule
exp011: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule (longer )
exp012: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule (even longer)    -->  8.10%
exp013: exp012 + reinitWeightsOnStep                                   --> goes to shit (>30%) / too slow
exp014: exp012 + no quantization for layer 1                           -->  7.25%
exp015: INQ, fixed LR, INQ schedule w/ cont. exp. decay                -->  8.03%
exp016: exp014 + CrossEntropyLoss instead of custom HingeLoss           --> incomplete

exp020: TAN, lr=1e-3, lrSchedule                                       -->  8.48%

exp030: TNN, act STE, wght STE, lrSchedule                             -->  9.96%
exp031: TNN, act STE, wght INQ 2-bit                                   -->  9.70%

exp040: magn-SRQ, quantAll, ramp-up, repartition every 100ep           --> incomplete (to 90% only), looking good (6.94%)
exp041: magn-SRQ, quantAll, ramp-up, repartition every 100ep, to full  -->  8.95% (last step too hard, 7.95%@95%)
exp042: magn-SRQ, quantAll, 90% quant always, repart. all 100ep        --> incomplete (90% only), far too slow

also added smooth transition between samplings (i.e. keeping full-prec weights underneath)!!!
exp066: magn-SRQ, quantAll, warm-up 150 ep., ramp-up, repart. all 1ep  -->  6.97%
exp067: magn-SRQ, quantAll, warm-up  20 ep., ramp-up, repart. all 1ep  -->  7.43%

exp069: faster ramp-up, added features (graph, inq/fraction), skip 1st -->  6.56%

exp070: exp069 pretr., super-fast schedule, 0.1x LR than exp069
exp071: exp069 pretr., super-fast schedule, same LR than exp069
exp072: exp069 pretr., crazy-fast schedule, same LR than exp069





Could also be combined with STE (bad idea, right?!)
Re-add skipping quantization of the last layer!!!
Also: rethink initial transition?!
Also: rethink label smoothing by trained teacher network
Also: re-extension to BNNs?
dLAC-style loss function
BWN/BNN instead of TWN/TNN?

summary:
- normal full-prec: 6.4%
- normal INQ: 7.3%



tensorboard filters: 
- CIFAR-10/logs/exp0([3-8]|0)



grouped weights annealing?!?! or even just otherwise a smoother transition....
higher learning rates after partial quantization?!?!
loss function of dLAC for max capacity (or transfer learning with smoothened ground truth)


partition-wise retraining
INQ controller
dLAC loss: only gradients when needed (use capacity better?!)
