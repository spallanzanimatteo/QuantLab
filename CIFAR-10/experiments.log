exp000: full-precision, lr=1e-3, lrSchedule (cyclic)                   -->  6.40%

exp010: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule
exp011: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule (longer )
exp012: TWN, INQ, 2-bit, lr=1e-3, inq- and lrSchedule (even longer)    -->  8.10%
exp013: exp012 + reinitWeightsOnStep                                   --> goes to shit (>30%) / too slow
exp014: exp012 + no quantization for layer 1                           -->  7.25%
exp015: INQ, fixed LR, INQ schedule w/ cont. exp. decay                -->  8.03%
exp016: exp014 + CrossEntropyLoss instead of custom HingeLoss          --> incomplete

exp020: TAN, lr=1e-3, lrSchedule, ANA                                  -->  8.48%
exp021: TAN, lr=1e-3, lrSchedule, STEcust                              -->  8.70%

exp030: TNN, act STE, wght STE, lrSchedule                             -->  9.96%
exp031: TNN, act STE, wght INQ 2-bit                                   -->  9.70%

exp040: magn-SRQ, quantAll, ramp-up, repartition every 100ep           --> incomplete (to 90% only), looking good (6.94%)
exp041: magn-SRQ, quantAll, ramp-up, repartition every 100ep, to full  -->  8.95% (last step too hard, 7.95%@95%)
exp042: magn-SRQ, quantAll, 90% quant always, repart. all 100ep        --> incomplete (90% only), far too slow

also added smooth transition between samplings (i.e. keeping full-prec weights underneath)!!!
exp066: magn-SRQ, quantAll, warm-up 150 ep., ramp-up, repart. all 1ep  -->  6.97%
exp067: magn-SRQ, quantAll, warm-up  20 ep., ramp-up, repart. all 1ep  -->  7.43%

exp069: faster ramp-up, added features (graph, inq/fraction), skip 1st -->  6.56%

exp070: exp069 pretr., super-fast schedule, 0.1x LR than exp069        -->  6.90%
exp071: exp069 pretr., super-fast schedule, same LR than exp069        -->  6.70%
exp072: exp069 pretr., crazy-fast schedule, same LR than exp069        -->  6.90%
exp073: exp069 pretr., super-fast schedule, same LR than exp069, numBit=3   6.48%

exp076: exp021 pretr.->STEcust+SRQ  --> 9.00%

exp078: + per-batch SRQ (slow due to frequent quantization!)

079 : BUG!!
exp080: exp021 + BAN instead of TAN; new STEcust impl.
exp083: exp071 + BWN instead of TWN
exp084: exp071 + BWN instead of TWN, quant to s/2
exp085: exp071 + BWN instead of TWN, quant to s/2; s=2*median(abs(w))
exp087: full-precision, lr=1e-3, lrSchedule (cyclic)
exp089: exp071 + BWN instead of TWN + 10x LR                           -->  7.58%
exp090: exp071 + BWN instead of TWN + first 5 eps at 10x LR            -->  6.82%
#!!!! experiments 085 failes, because s is already determined, also exp084 affected !!!!


Could also be combined with STE (bad idea, right?!)
Also: rethink initial transition?! smoother?
Also: rethink label smoothing by trained teacher network
Also: re-extension to BNNs?
dLAC-style loss function
BWN/BNN instead of TWN/TNN?

summary:
- normal full-prec: 6.4%
- normal INQ: 7.3%



tensorboard filters: 
- CIFAR-10/logs/exp0([3-8]|0)



grouped weights annealing?!?! or even just otherwise a smoother transition....
higher learning rates after partial quantization?!?!
loss function of dLAC for max capacity (or transfer learning with smoothened ground truth)


partition-wise retraining
INQ controller
dLAC loss: only gradients when needed (use capacity better?!)
